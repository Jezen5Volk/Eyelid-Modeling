{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data_management import Mat2TVT, Preprocessor, Custom_EMG, Jitter, MaskRand\n",
    "from models.RNN_rect import EMG_RNN\n",
    "from utils.training_overhead import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Reorganization** <br>\n",
    "Reorganizing provided matlab file and interpolating over recoverable NaN labels\n",
    "\n",
    "For a single trial, \n",
    "\n",
    "- Marker kinematics are provided at 400 Hz, shape (300, 3) --> 0.75 s\n",
    "- EMG information is provided at 6103.5 Hz, shape (4564,) --> 0.7478 s\n",
    "\n",
    "Marker and EMG data are time-synced. Blinking starts at around 0.25 seconds, which corresponds to point 100 for the kinematics data and point 1525 in the EMG data\n",
    "\n",
    "Since only electrodes u1, u2, u3, u4, and t2 were used in common across all participants, our input to the model (without spectral preprocessing) will be of shape (4564, N, 5). Since only markerset x1, x2, A, B, C, D, E were used in common across all participants, our labels will be of shape (N, 7, 300, 3). In other words, $x \\in R^{T_{\\mathrm{in}}\\times N \\times C_{\\mathrm{in}}}$, where $T_{\\mathrm{in}}$ is the number of timesteps (4564, in this case), N is the batch size, and $C_{\\mathrm{in}}$ is the number of channels (5, in this case), while $y \\in R^{T_{\\mathrm{out}}\\times 3\\times N \\times C_{\\mathrm{out}}}$, where where $T_{\\mathrm{out}}$ is the number of timesteps (300 x 3, in this case), N is the batch size, and $C_{\\mathrm{out}}$ is the number of channels (7, in this case).\n",
    "\n",
    "To reorganize the data for train-test-val splits, the Nth data/label pair should contain the timeseries for emg electrodes u1, u2, u3, u4, t2/the timeseries for markerset x1, x2, A, B, C, D, E. In addition to this, a static identification label is generated which contains an identifying string combining the subject, blink type, and trial number (ie: 'sub1_spon#23'), a boolean hyperparameter indicating eye side (True = Right Eye, False = Left Eye); and a flag to identify whether the trial contains NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (4564, 977, 5)\n",
      "Label Shape:  (300, 3, 977, 5)\n",
      "Identifier Shape:  (977, 3)\n",
      "Shapes Validated!\n"
     ]
    }
   ],
   "source": [
    "eye_bool = np.asarray([True, True, False, True, True, False, False, False])\n",
    "electrode_list = ['u1', 'u2', 'u3', 'u4', 't2']\n",
    "marker_list = ['u1', 'u2', 'u3', 'u4', 'u5']\n",
    "filepath = 'eyeliddata.mat'\n",
    "\n",
    "train = 0.7\n",
    "val = 0.2\n",
    "test = 0.1\n",
    "\n",
    "reorganizer = Mat2TVT(eye_bool, electrode_list, marker_list, filepath)\n",
    "_, _, identifier = reorganizer.load_data()\n",
    "X, y = reorganizer.DMVC_norm()\n",
    "TVT_dict = reorganizer.TVT_split(train, val, test)\n",
    "\n",
    "\n",
    "X_train, y_train = TVT_dict[\"X_train\"], TVT_dict[\"y_train\"]\n",
    "X_val, y_val = TVT_dict[\"X_val\"], TVT_dict[\"y_val\"]\n",
    "X_test, y_test = TVT_dict[\"X_test\"], TVT_dict[\"y_test\"]\n",
    "\n",
    "'''\n",
    "Shape Validation\n",
    "'''\n",
    "#Validate shapes\n",
    "print(\"Data Shape: \", X.shape)\n",
    "print(\"Label Shape: \", y.shape)\n",
    "print(\"Identifier Shape: \", identifier.shape)\n",
    "assert X.shape[0] == 4564, \"X Shape Invalid: Should have 4564 timesteps\"\n",
    "assert X.shape[2] == len(electrode_list), \"X Shape Invalid: Incorrect channel count\"\n",
    "assert y.shape[0] == 300, \"y Shape Invalid: Should have 300 timesteps\"\n",
    "assert y.shape[1] == 3, \"y Shape Invalid: Each timestep should have 3 (cartesian) dimensions\"\n",
    "assert y.shape[3] == len(marker_list), \"y Shape Invalid: Incorrect channel count\"\n",
    "assert X.shape[1] == y.shape[2] == identifier.shape[0], \"Shapes Invalid: The trial dimension of X, y, and identifier needs to match\"\n",
    "print(\"Shapes Validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models** <br>\n",
    "\n",
    "t is worth pointing out that the data I have is not surface EMG data but data collected by needles inserted into the obicularis muscle. This means that there is finer resolution into picking up signals from individual motor units, which means that it is possible that there is enough information captured in the rectified EMG signal to recreate eyelid motions, without adding a pre-processing step to aquire the spectral form of the data. The ultimate goal is performance, which means that it is worth testing the model architectures with rectified EMG inputs vs spectral encoding, as well as if there is extra time, a combined model architecture which would accept spectral and rectified EMG. \n",
    "\n",
    "**Preprocessing**: Since it is impossible to insert needle electrodes into exactly the same location on the obicularis for each subject, and individuals have variance in their ability to activate their muscles, the first step of data preprocessing is to perform dynamic maximum voluntary contraction normalization on the EMG data, where we normalize to unit statistics each channel on each subject. Because we expect the relative eye motion to be important (rather than the absolute location of the eye motion), all of the labels are shifted so that their minimum value lies at y = 0. This should hopefully enable the model to learn the desired shape of the eyelid trajectory across subjects without obfuscation(ie: a blink whose x coordinate moves from -4 to -6 is the same type of blink as one whose x coordinate moves from -8 to -10). \n",
    "\n",
    "Since the desired application is a real-time model of eyelid kinematics directly informed by the EMG behavior of the obicularis muscle, we need to break our data into windows. Thus, window length and window stride will end up being hyperparameters that we optimize for with Ray Tune, and for simplicity we'll use 'same' padding. This means that our input to the model will actually be of shape $X \\in R^{L \\times N_w \\times N_b \\times C}$ where $L$ is the length of our window, $N_w$ is the number of windows in a batch, $N_b$ is the number of examples in a batch, and $C$ is the number of channels present in the data (in this case, 5). Once the data is windowed, the next step in preprocessing is to either identify the frquency spectrum or to rectify the data. \n",
    "\n",
    "**Data Augmentation**: timeseries jitter + SpecAugment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122, 68, 5]) torch.Size([4, 3, 68, 5])\n",
      "Feature batch shape: torch.Size([64, 122, 68, 5])\n",
      "Labels batch shape: torch.Size([64, 4, 3, 68, 5])\n",
      "cuda:0 cuda:0\n"
     ]
    }
   ],
   "source": [
    "t_win = 20\n",
    "t_lookahead = 50\n",
    "t_stride = 10\n",
    "batch_size = 64\n",
    "p_transform = 0.3\n",
    "sigma = 0.3\n",
    "p_mask = 0.1\n",
    "\n",
    "#Window and rectify the EMG data\n",
    "preprocessor = Preprocessor(t_win, t_lookahead, t_stride)\n",
    "X_train_wr, y_train_wr = preprocessor.win_rect(X_train, y_train)\n",
    "X_val_wr, y_val_wr = preprocessor.win_rect(X_val, y_val)\n",
    "\n",
    "#Load into custom torch.Dataset object, which applies our data augmentation (Jitter, random masking)\n",
    "transform = v2.RandomApply(torch.nn.ModuleList([Jitter(sigma), MaskRand(p_mask)]), p = p_transform)\n",
    "\n",
    "train_data = Custom_EMG(X_train_wr, y_train_wr, transform = transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle = True)\n",
    "\n",
    "val_data = Custom_EMG(X_val_wr, y_val_wr, transform = transform)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trial, label = train_data.__getitem__(0)\n",
    "print(trial.shape, label.shape)\n",
    "\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")\n",
    "print(train_features.device, train_labels.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 8.632693  [   64/  683]\n",
      "loss: 9.466209  [  384/  683]\n",
      "loss: 8.163328  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 233.6%, Avg Marker Error: 93.8%, Avg loss: 9.406903 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 8.721226  [   64/  683]\n",
      "loss: 8.713776  [  384/  683]\n",
      "loss: 9.367476  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 253.7%, Avg Marker Error: 94.4%, Avg loss: 8.359723 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 8.738490  [   64/  683]\n",
      "loss: 7.831609  [  384/  683]\n",
      "loss: 9.932140  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 260.0%, Avg Marker Error: 94.3%, Avg loss: 9.045644 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 7.912688  [   64/  683]\n",
      "loss: 8.554914  [  384/  683]\n",
      "loss: 9.955528  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 201.1%, Avg Marker Error: 92.9%, Avg loss: 8.558737 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 8.879265  [   64/  683]\n",
      "loss: 8.448089  [  384/  683]\n",
      "loss: 8.907121  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 215.4%, Avg Marker Error: 94.8%, Avg loss: 10.618849 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 9.161122  [   64/  683]\n",
      "loss: 8.387556  [  384/  683]\n",
      "loss: 7.573293  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 249.8%, Avg Marker Error: 94.6%, Avg loss: 8.649342 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 8.844357  [   64/  683]\n",
      "loss: 9.097937  [  384/  683]\n",
      "loss: 8.426313  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 263.8%, Avg Marker Error: 93.3%, Avg loss: 8.423333 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 8.562864  [   64/  683]\n",
      "loss: 9.500471  [  384/  683]\n",
      "loss: 9.182491  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 230.4%, Avg Marker Error: 92.7%, Avg loss: 9.278244 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 9.517447  [   64/  683]\n",
      "loss: 8.515832  [  384/  683]\n",
      "loss: 8.777848  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 257.7%, Avg Marker Error: 93.3%, Avg loss: 10.113756 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 8.170142  [   64/  683]\n",
      "loss: 9.244944  [  384/  683]\n",
      "loss: 10.397144  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 153.9%, Avg Marker Error: 94.6%, Avg loss: 9.830043 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 8.229215  [   64/  683]\n",
      "loss: 8.185984  [  384/  683]\n",
      "loss: 8.806814  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 187.8%, Avg Marker Error: 92.2%, Avg loss: 8.259290 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 8.768837  [   64/  683]\n",
      "loss: 8.632179  [  384/  683]\n",
      "loss: 7.735084  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 271.8%, Avg Marker Error: 94.7%, Avg loss: 9.436738 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 8.576116  [   64/  683]\n",
      "loss: 8.336940  [  384/  683]\n",
      "loss: 7.814345  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 243.5%, Avg Marker Error: 94.4%, Avg loss: 8.975814 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 8.616399  [   64/  683]\n",
      "loss: 9.248906  [  384/  683]\n",
      "loss: 9.234885  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 255.6%, Avg Marker Error: 95.4%, Avg loss: 9.617767 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 7.919226  [   64/  683]\n",
      "loss: 8.593976  [  384/  683]\n",
      "loss: 9.823210  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 258.3%, Avg Marker Error: 93.6%, Avg loss: 8.877642 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 10.032104  [   64/  683]\n",
      "loss: 8.494028  [  384/  683]\n",
      "loss: 8.472706  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 235.9%, Avg Marker Error: 94.8%, Avg loss: 9.082270 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 8.647092  [   64/  683]\n",
      "loss: 8.761644  [  384/  683]\n",
      "loss: 9.045451  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 230.4%, Avg Marker Error: 94.1%, Avg loss: 8.803569 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 8.200604  [   64/  683]\n",
      "loss: 8.796658  [  384/  683]\n",
      "loss: 9.559077  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 209.5%, Avg Marker Error: 92.7%, Avg loss: 9.110245 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 8.023680  [   64/  683]\n",
      "loss: 9.997261  [  384/  683]\n",
      "loss: 9.678648  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 245.3%, Avg Marker Error: 94.5%, Avg loss: 9.482253 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 8.251493  [   64/  683]\n",
      "loss: 9.286187  [  384/  683]\n",
      "loss: 9.352170  [  683/  683]\n",
      "Validation Error: \n",
      " Max Marker Error: 234.6%, Avg Marker Error: 92.6%, Avg loss: 8.333701 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "dropout = 0.3\n",
    "learning_rate = 1e-6\n",
    "epochs = 20\n",
    "\n",
    "model = EMG_RNN(train_features.size(), train_labels.size(), 2, dropout)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(train_dataloader, val_dataloader, model, loss_fn, optimizer, batch_size, epochs)\n",
    "model_weights = trainer.train()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
