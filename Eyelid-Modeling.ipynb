{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data_management import Mat2TVT, Preprocessor, Custom_EMG, Jitter, MaskRand\n",
    "from models.RNN_rect import EMG_RNN\n",
    "from utils.training_overhead import Trainer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Reorganization** <br>\n",
    "Reorganizing provided matlab file and interpolating over recoverable NaN labels\n",
    "\n",
    "For a single trial, \n",
    "\n",
    "- Marker kinematics are provided at 400 Hz, shape (300, 3) --> 0.75 s\n",
    "- EMG information is provided at 6103.5 Hz, shape (4564,) --> 0.7478 s\n",
    "\n",
    "Marker and EMG data are time-synced. Blinking starts at around 0.25 seconds, which corresponds to point 100 for the kinematics data and point 1525 in the EMG data\n",
    "\n",
    "Since only electrodes u1, u2, u3, u4, and t2 were used in common across all participants, our input to the model (without spectral preprocessing) will be of shape (4564, N, 5). Since only markerset x1, x2, A, B, C, D, E were used in common across all participants, our labels will be of shape (N, 7, 300, 3). In other words, $x \\in R^{T_{\\mathrm{in}}\\times N \\times C_{\\mathrm{in}}}$, where $T_{\\mathrm{in}}$ is the number of timesteps (4564, in this case), N is the batch size, and $C_{\\mathrm{in}}$ is the number of channels (5, in this case), while $y \\in R^{T_{\\mathrm{out}}\\times 3\\times N \\times C_{\\mathrm{out}}}$, where where $T_{\\mathrm{out}}$ is the number of timesteps (300 x 3, in this case), N is the batch size, and $C_{\\mathrm{out}}$ is the number of channels (7, in this case).\n",
    "\n",
    "To reorganize the data for train-test-val splits, the Nth data/label pair should contain the timeseries for emg electrodes u1, u2, u3, u4, t2/the timeseries for markerset x1, x2, A, B, C, D, E. In addition to this, a static identification label is generated which contains an identifying string combining the subject, blink type, and trial number (ie: 'sub1_spon#23'), a boolean hyperparameter indicating eye side (True = Right Eye, False = Left Eye); and a flag to identify whether the trial contains NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (4564, 977, 5)\n",
      "Label Shape:  (300, 3, 977, 5)\n",
      "Identifier Shape:  (977, 3)\n",
      "Shapes Validated!\n"
     ]
    }
   ],
   "source": [
    "eye_bool = np.asarray([True, True, False, True, True, False, False, False])\n",
    "electrode_list = ['u1', 'u2', 'u3', 'u4', 't2']\n",
    "marker_list = ['u1', 'u2', 'u3', 'u4', 'u5']\n",
    "filepath = 'eyeliddata.mat'\n",
    "\n",
    "train = 0.7\n",
    "val = 0.2\n",
    "test = 0.1\n",
    "\n",
    "reorganizer = Mat2TVT(eye_bool, electrode_list, marker_list, filepath)\n",
    "_, _, identifier = reorganizer.load_data()\n",
    "X, y = reorganizer.DMVC_norm()\n",
    "TVT_dict = reorganizer.TVT_split(train, val, test)\n",
    "\n",
    "\n",
    "X_train, y_train = TVT_dict[\"X_train\"], TVT_dict[\"y_train\"]\n",
    "X_val, y_val = TVT_dict[\"X_val\"], TVT_dict[\"y_val\"]\n",
    "X_test, y_test = TVT_dict[\"X_test\"], TVT_dict[\"y_test\"]\n",
    "\n",
    "'''\n",
    "Shape Validation\n",
    "'''\n",
    "#Validate shapes\n",
    "print(\"Data Shape: \", X.shape)\n",
    "print(\"Label Shape: \", y.shape)\n",
    "print(\"Identifier Shape: \", identifier.shape)\n",
    "assert X.shape[0] == 4564, \"X Shape Invalid: Should have 4564 timesteps\"\n",
    "assert X.shape[2] == len(electrode_list), \"X Shape Invalid: Incorrect channel count\"\n",
    "assert y.shape[0] == 300, \"y Shape Invalid: Should have 300 timesteps\"\n",
    "assert y.shape[1] == 3, \"y Shape Invalid: Each timestep should have 3 (cartesian) dimensions\"\n",
    "assert y.shape[3] == len(marker_list), \"y Shape Invalid: Incorrect channel count\"\n",
    "assert X.shape[1] == y.shape[2] == identifier.shape[0], \"Shapes Invalid: The trial dimension of X, y, and identifier needs to match\"\n",
    "print(\"Shapes Validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models** <br>\n",
    "\n",
    "There are three different model architectures that I think would be important to explore for the purposes of this project: RNNs, LSTMs, and GRUs. In addition to these model architectures, it is worth pointing out that the data I have is not surface EMG data but data collected by needles inserted into the obicularis muscle. This means that there is finer resolution into picking up signals from individual motor units, which means that it is possible that there is enough information captured in the rectified EMG signal to recreate eyelid motions, without adding a pre-processing step to aquire the spectral form of the data. The ultimate goal is performance, which means that it is worth testing the model architectures with rectified EMG inputs vs spectral encoding, as well as if there is extra time, a combined model architecture which would accept spectral and rectified EMG. \n",
    "\n",
    "**Preprocessing**: Since it is impossible to insert needle electrodes into exactly the same location on the obicularis for each subject, and individuals have variance in their ability to activate their muscles, the first step of data preprocessing is to perform dynamic maximum voluntary contraction normalization on the EMG data, where we normalize to unit statistics each channel on each subject. Because we expect the relative eye motion to be important (rather than the absolute location of the eye motion), all of the labels are shifted so that their minimum value lies at y = 0. This should hopefully enable the model to learn the desired shape of the eyelid trajectory across subjects without obfuscation(ie: a blink whose x coordinate moves from -4 to -6 is the same type of blink as one whose x coordinate moves from -8 to -10). \n",
    "\n",
    "Since the desired application is a real-time model of eyelid kinematics directly informed by the EMG behavior of the obicularis muscle, we need to break our data into windows. Thus, window length and window stride will end up being hyperparameters that we optimize for with Ray Tune, and for simplicity we'll use 'same' padding. This means that our input to the model will actually be of shape $X \\in R^{L \\times N_w \\times N_b \\times C}$ where $L$ is the length of our window, $N_w$ is the number of windows in a batch, $N_b$ is the number of examples in a batch, and $C$ is the number of channels present in the data (in this case, 5). Once the data is windowed, the next step in preprocessing is to either identify the frquency spectrum or to rectify the data. \n",
    "\n",
    "**Data Augmentation**: worry about that when ya get there, but timeseries jitter + SpecAugment\n",
    "\n",
    "**Model Architectures**: WORRY ABOUT THAT WHEN YA GET THERE\n",
    "\n",
    "**Training**: it's all downhill from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([122, 5]) torch.Size([4, 3, 5])\n",
      "Feature batch shape: torch.Size([64, 122, 5])\n",
      "Labels batch shape: torch.Size([64, 4, 3, 5])\n"
     ]
    }
   ],
   "source": [
    "t_win = 20\n",
    "t_lookahead = 50\n",
    "t_stride = 10\n",
    "batch_size = 64\n",
    "p_transform = 0\n",
    "sigma = 0.3\n",
    "p_mask = 0.1\n",
    "\n",
    "#Window and rectify the EMG data\n",
    "preprocessor = Preprocessor(t_win, t_lookahead, t_stride)\n",
    "X_train_wr, y_train_wr = preprocessor.win_rect(X_train, y_train)\n",
    "X_val_wr, y_val_wr = preprocessor.win_rect(X_val, y_val)\n",
    "\n",
    "\n",
    "#Load into custom torch.Dataset object, which applies our data augmentation (Jitter, random masking)\n",
    "transform = v2.RandomApply(torch.nn.ModuleList([Jitter(sigma), MaskRand(p_mask)]), p = p_transform)\n",
    "\n",
    "train_data = Custom_EMG(X_train_wr, y_train_wr, transform = transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle = True)\n",
    "\n",
    "val_data = Custom_EMG(X_val_wr, y_val_wr, transform = transform)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle = True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "trial, label = train_data.__getitem__(0)\n",
    "print(trial.shape, label.shape)\n",
    "\n",
    "\n",
    "train_features, train_labels = next(iter(train_dataloader))\n",
    "print(f\"Feature batch shape: {train_features.size()}\")\n",
    "print(f\"Labels batch shape: {train_labels.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 9.189815  [   64/46444]\n",
      "loss: 7.616344  [ 6464/46444]\n",
      "loss: 10.567101  [12864/46444]\n",
      "loss: 8.962626  [19264/46444]\n",
      "loss: 10.541308  [25664/46444]\n",
      "loss: 8.744366  [32064/46444]\n",
      "loss: 7.987884  [38464/46444]\n",
      "loss: 7.779994  [44864/46444]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (44) must match the size of tensor b (64) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n\u001b[32m      9\u001b[39m trainer = Trainer(train_dataloader, val_dataloader, model, loss_fn, optimizer, batch_size, epochs)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m model_weights = \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\OneDrive\\Documents\\GitHub\\Eyelid-Modeling\\utils\\training_overhead.py:18\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, verbose)\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.epochs):\n\u001b[32m     17\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m-------------------------------\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m     \u001b[38;5;28mself\u001b[39m.val_loop()\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mDone!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:7\u001b[39m, in \u001b[36mtrain_loop\u001b[39m\u001b[34m(self)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\.conda\\envs\\pytorch_GPU\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\.conda\\envs\\pytorch_GPU\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:24\u001b[39m, in \u001b[36mforward\u001b[39m\u001b[34m(self, X, pred)\u001b[39m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (44) must match the size of tensor b (64) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "dropout = 0.3\n",
    "learning_rate = 1e-6\n",
    "epochs = 2\n",
    "\n",
    "model = EMG_RNN(train_features.size(), train_labels.size(), 2, dropout)\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "trainer = Trainer(train_dataloader, val_dataloader, model, loss_fn, optimizer, batch_size, epochs)\n",
    "model_weights = trainer.train(verbose = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
