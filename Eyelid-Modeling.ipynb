{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Import Statements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision.transforms import v2\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.data_management import Mat2TVT, Preprocessor, Custom_EMG, Jitter, MaskRand\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Data Reorganization** <br>\n",
    "Reorganizing provided matlab file and interpolating over recoverable NaN labels\n",
    "\n",
    "For a single trial, \n",
    "\n",
    "- Marker kinematics are provided at 400 Hz, shape (300, 3) --> 0.75 s\n",
    "- EMG information is provided at 6103.5 Hz, shape (4564,) --> 0.7478 s\n",
    "\n",
    "Marker and EMG data are time-synced. Blinking starts at around 0.25 seconds, which corresponds to point 100 for the kinematics data and point 1525 in the EMG data\n",
    "\n",
    "Since only electrodes u1, u2, u3, u4, and t2 were used in common across all participants, our input to the model (without spectral preprocessing) will be of shape (4564, N, 5). Since only markerset x1, x2, A, B, C, D, E were used in common across all participants, our labels will be of shape (N, 7, 300, 3). In other words, $x \\in R^{T_{\\mathrm{in}}\\times N \\times C_{\\mathrm{in}}}$, where $T_{\\mathrm{in}}$ is the number of timesteps (4564, in this case), N is the batch size, and $C_{\\mathrm{in}}$ is the number of channels (5, in this case), while $y \\in R^{T_{\\mathrm{out}}\\times 3\\times N \\times C_{\\mathrm{out}}}$, where where $T_{\\mathrm{out}}$ is the number of timesteps (300 x 3, in this case), N is the batch size, and $C_{\\mathrm{out}}$ is the number of channels (7, in this case).\n",
    "\n",
    "To reorganize the data for train-test-val splits, the Nth data/label pair should contain the timeseries for emg electrodes u1, u2, u3, u4, t2/the timeseries for markerset x1, x2, A, B, C, D, E. In addition to this, a static identification label is generated which contains an identifying string combining the subject, blink type, and trial number (ie: 'sub1_spon#23'), a boolean hyperparameter indicating eye side (True = Right Eye, False = Left Eye); and a flag to identify whether the trial contains NaN values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Shape:  (4564, 986, 5)\n",
      "Label Shape:  (300, 3, 986, 7)\n",
      "Identifier Shape:  (986, 3)\n",
      "Shapes Validated!\n"
     ]
    }
   ],
   "source": [
    "eye_bool = np.asarray([True, True, False, True, True, False, False, False])\n",
    "electrode_list = ['u1', 'u2', 'u3', 'u4', 't2']\n",
    "marker_list = ['x1', 'x2', 'u1', 'u2', 'u3', 'u4', 'u5']\n",
    "filepath = 'eyeliddata.mat'\n",
    "\n",
    "train = 0.7\n",
    "val = 0.2\n",
    "test = 0.1\n",
    "\n",
    "reorganizer = Mat2TVT(eye_bool, electrode_list, marker_list, filepath)\n",
    "_, _, identifier = reorganizer.load_data()\n",
    "X, y = reorganizer.DMVC_norm()\n",
    "TVT_dict = reorganizer.TVT_split(train, val, test)\n",
    "\n",
    "\n",
    "X_train, y_train = TVT_dict[\"X_train\"], TVT_dict[\"y_train\"]\n",
    "X_val, y_val = TVT_dict[\"X_val\"], TVT_dict[\"y_val\"]\n",
    "X_test, y_test = TVT_dict[\"X_test\"], TVT_dict[\"y_test\"]\n",
    "\n",
    "'''\n",
    "Shape Validation\n",
    "'''\n",
    "#Validate shapes\n",
    "print(\"Data Shape: \", X.shape)\n",
    "print(\"Label Shape: \", y.shape)\n",
    "print(\"Identifier Shape: \", identifier.shape)\n",
    "assert X.shape[0] == 4564, \"X Shape Invalid: Should have 4564 timesteps\"\n",
    "assert X.shape[2] == 5, \"X Shape Invalid: Should have 5 Channels\"\n",
    "assert y.shape[0] == 300, \"y Shape Invalid: Should have 300 timesteps\"\n",
    "assert y.shape[1] == 3, \"y Shape Invalid: Each timestep should have 3 (cartesian) dimensions\"\n",
    "assert y.shape[3] == 7, \"y Shape Invalid: Should have 7 Channels\"\n",
    "assert X.shape[1] == y.shape[2] == identifier.shape[0], \"Shapes Invalid: The trial dimension of X, y, and identifier needs to match\"\n",
    "print(\"Shapes Validated!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Models** <br>\n",
    "\n",
    "There are three different model architectures that I think would be important to explore for the purposes of this project: RNNs, LSTMs, and GRUs. In addition to these model architectures, it is worth pointing out that the data I have is not surface EMG data but data collected by needles inserted into the obicularis muscle. This means that there is finer resolution into picking up signals from individual motor units, which means that it is possible that there is enough information captured in the rectified EMG signal to recreate eyelid motions, without adding a pre-processing step to aquire the spectral form of the data. The ultimate goal is performance, which means that it is worth testing the model architectures with rectified EMG inputs vs spectral encoding, as well as if there is extra time, a combined model architecture which would accept spectral and rectified EMG. \n",
    "\n",
    "**Preprocessing**: Since it is impossible to insert needle electrodes into exactly the same location on the obicularis for each subject, and individuals have variance in their ability to activate their muscles, the first step of data preprocessing is to perform dynamic maximum voluntary contraction normalization on the EMG data, where we normalize to unit statistics each channel on each subject. Because we expect the relative eye motion to be important (rather than the absolute location of the eye motion), all of the labels are shifted so that their minimum value lies at y = 0. This should hopefully enable the model to learn the desired shape of the eyelid trajectory across subjects without obfuscation(ie: a blink whose x coordinate moves from -4 to -6 is the same type of blink as one whose x coordinate moves from -8 to -10). \n",
    "\n",
    "Since the desired application is a real-time model of eyelid kinematics directly informed by the EMG behavior of the obicularis muscle, we need to break our data into windows. Thus, window length and window stride will end up being hyperparameters that we optimize for with Ray Tune, and for simplicity we'll use 'same' padding. This means that our input to the model will actually be of shape $X \\in R^{L \\times N_w \\times N_b \\times C}$ where $L$ is the length of our window, $N_w$ is the number of windows in a batch, $N_b$ is the number of examples in a batch, and $C$ is the number of channels present in the data (in this case, 5). Once the data is windowed, the next step in preprocessing is to either identify the frquency spectrum or to rectify the data. \n",
    "\n",
    "**Data Augmentation**: worry about that when ya get there, but timeseries jitter + SpecAugment\n",
    "\n",
    "**Model Architectures**: WORRY ABOUT THAT WHEN YA GET THERE\n",
    "\n",
    "**Training**: it's all downhill from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_win = 20\n",
    "t_lookahead = 50\n",
    "t_stride = 10\n",
    "batch_size = 64\n",
    "p_transform = 0.3\n",
    "sigma = 0.3\n",
    "p_mask = 0.1\n",
    "\n",
    "#Window and rectify the EMG data\n",
    "preprocessor = Preprocessor(t_win, t_lookahead, t_stride)\n",
    "X_train_wr, y_train_wr = preprocessor.win_rect(X_train, y_train)\n",
    "X_val_wr, y_val_wr = preprocessor.win_rect(X_val, y_val)\n",
    "\n",
    "\n",
    "#Load into custom torch.Dataset object, which applies our data augmentation (Jitter, random masking)\n",
    "transform = v2.RandomApply([Jitter(sigma), MaskRand(p_mask)], p = p_transform)\n",
    "\n",
    "train_data = Custom_EMG(X_train_wr, y_train_wr, transform = transform)\n",
    "train_dataloader = DataLoader(train_data, batch_size, shuffle = True)\n",
    "\n",
    "val_data = Custom_EMG(X_val_wr, y_val_wr, transform = transform)\n",
    "val_dataloader = DataLoader(val_data, batch_size, shuffle = True)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_GPU",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
